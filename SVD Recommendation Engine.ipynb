{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binary-champagne",
   "metadata": {},
   "source": [
    "# Recommending movies using Truncated SVD\n",
    "#### There are latent features in the reconstructed matrix showing a correlation with the user ratings for the rating prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "apart-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ambient-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p1,p2):\n",
    "    distance = math.sqrt( ((p1[0]-p2[0])**2)+((p1[1]-p2[1])**2) )\n",
    "    return distance \n",
    "\n",
    "def corr_percentage(df, set1,set2):\n",
    "    test = df[df['Set'] == set1]\n",
    "    #print(test.head(10))\n",
    "    confirm = df[df.UserID.isin(test.UserID.unique())]\n",
    "    #print(confirm.head(10))\n",
    "    confirm = confirm[confirm['Set']== set2]\n",
    "    return len(confirm.UserID.unique())/len(test.UserID.unique())\n",
    "\n",
    "\n",
    "def correlation_fun(df):\n",
    "    \"\"\"\n",
    "    df -> dataframe: the dataframe to be analyzed by svd recommendation engine\n",
    "          with 3 columns, UserID, Set, Rating\n",
    "    \"\"\"\n",
    "    df_user_le = pd.DataFrame(df[\"UserID\"].tolist(), columns=['user'])\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df_user_le['user_le'] = le.fit_transform(df_user_le['user'])\n",
    "    df_set_le = pd.DataFrame(df[\"Set\"].tolist(), columns=['set'])\n",
    "    df_set_le['set_le'] = le.fit_transform(df_set_le['set'])\n",
    "\n",
    "    user_ids = np.array(df_user_le['user_le'].tolist())\n",
    "    set_ids = np.array(df_set_le['set_le'].tolist())\n",
    "    rating_ids = np.array(df[\"Rating\"].tolist())\n",
    "    n_user = df['UserID'].nunique()\n",
    "    n_set = df['Set'].nunique()\n",
    "    print ('num of users:',df['UserID'].nunique())\n",
    "    print ('num of sets:',df['Set'].nunique())\n",
    "    print (rating_ids.shape[0])\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "\n",
    "\n",
    "    embedding_size = 30\n",
    "\n",
    "    lr = 0.0001\n",
    "    reg = 0.01\n",
    "\n",
    "    with graph.as_default():\n",
    "        user = tf.placeholder(tf.int32, name=\"user_id\") \n",
    "        sets = tf.placeholder(tf.int32, name=\"set\") \n",
    "        rating = tf.placeholder(tf.float32, name=\"rating\") \n",
    "\n",
    "        set_embedding = tf.Variable(tf.truncated_normal([n_set, embedding_size], stddev=0.02, mean=0.) ,name=\"set_embedding\")\n",
    "        user_embedding = tf.Variable(tf.truncated_normal([n_user, embedding_size], stddev=0.02, mean=0.) ,name=\"user_embedding\")\n",
    "\n",
    "        set_bias_embedding = tf.Variable(tf.truncated_normal([n_set], stddev=0.02, mean=0.) ,name=\"set_bias_embedding\")\n",
    "        user_bias_embedding = tf.Variable(tf.truncated_normal([n_user], stddev=0.02, mean=0.) ,name=\"user_bias_embedding\")\n",
    "\n",
    "\n",
    "        global_bias = tf.Variable(tf.truncated_normal([], stddev=0.02, mean=0.) ,name=\"global_bias\")\n",
    "\n",
    "        u = tf.nn.embedding_lookup(user_embedding, user)\n",
    "        m = tf.nn.embedding_lookup(set_embedding, sets)\n",
    "\n",
    "        u_bias = tf.nn.embedding_lookup(user_bias_embedding, user)\n",
    "        m_bias = tf.nn.embedding_lookup(set_bias_embedding, sets)\n",
    "\n",
    "\n",
    "        predicted_rating = tf.reduce_sum(tf.multiply(u, m), 1) + u_bias + m_bias + global_bias\n",
    "\n",
    "        rmse = tf.sqrt(tf.reduce_mean(tf.square(predicted_rating - rating))) # RMSE\n",
    "        cost = tf.nn.l2_loss(predicted_rating - rating)\n",
    "        regularization = reg * (tf.nn.l2_loss(set_embedding) + tf.nn.l2_loss(user_embedding)\n",
    "                                + tf.nn.l2_loss(set_bias_embedding) + tf.nn.l2_loss(user_bias_embedding))\n",
    "\n",
    "        loss = cost + regularization\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        \n",
    "    # 2 numbers you can play with to fit your data more\n",
    "    batch_size = 5\n",
    "    n_epoch = 30\n",
    "\n",
    "    print(rating_ids.shape[0])\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for _ in range(n_epoch):\n",
    "            for start in range(0, rating_ids.shape[0] - batch_size, batch_size):\n",
    "                end = start + batch_size\n",
    "                _, cost_value = sess.run([optimizer, rmse], feed_dict={user: user_ids[start:end],\n",
    "                                                      sets: set_ids[start: end],\n",
    "                                                      rating: rating_ids[start: end]})\n",
    "\n",
    "#             print (\"RMSE\", cost_value)\n",
    "        embeddings = set_embedding.eval()\n",
    "    \n",
    "\n",
    "    df_set_fea_frame = pd.pivot_table(df, values='Rating', index=['Set'],columns=['UserID'], fill_value=0)\n",
    "    df_set_fea = pd.pivot_table(df, values='Rating', index=['Set'],columns=['UserID'], fill_value=0).to_numpy()\n",
    "\n",
    "    print ('{0}x{1} user by movie matrix'.format(*df_mv_fea.shape))\n",
    "    \n",
    "\n",
    "    df_rating1 = df.sample(n=df.shape[0], replace=False, random_state=1)\n",
    "    R = pd.pivot_table(df, values='Rating', index=['UserID'],columns=['Set'], fill_value=0).to_numpy()\n",
    "    # R = df_rating_mv[['userId','rating']].values\n",
    "    print ('{0}x{1} user by user matrix'.format(*R.shape))\n",
    "\n",
    "\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    import math\n",
    "    set_train_num = math.ceil(n_set)\n",
    "    user_train_num = math.ceil(n_user)\n",
    "    train_user = R[:, :]\n",
    "    #test_user = R[math.ceil(R.shape[0]*0.8):, math.ceil(R.shape[1]*0.8):]\n",
    "    train_set = df_set_fea[:, :]\n",
    "    #test_mv = df_mv_fea[math.ceil(df_mv_fea.shape[0]*0.8):, math.ceil(df_mv_fea.shape[1]*0.8):]\n",
    "\n",
    "    set_svd = TruncatedSVD(n_components=100)\n",
    "    set_features = set_svd.fit_transform(train_mv)\n",
    "\n",
    "    print (\"set_features.shape = {0}\".format(set_features.shape))\n",
    "\n",
    "\n",
    "    user_svd = TruncatedSVD(n_components=100)\n",
    "    user_features = user_svd.fit_transform(train_user)\n",
    "\n",
    "    print (\"user_features.shape = {0}\".format(user_features.shape))\n",
    "    \n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    tsne = TSNE(perplexity=5, n_components=2, init=\"pca\", n_iter=5000)\n",
    "    plot_only = 50\n",
    "    coords = tsne.fit_transform(embeddings[:,:])\n",
    "    print(len(coords))\n",
    "    #print(set_features)\n",
    "    plt.figure(figsize=(18, 18))\n",
    "#     labels = [df.iloc[i].Set for i in range(plot_only)]\n",
    "    labels = df.Set.to_list()\n",
    "    #print(labels)\n",
    "    already_cover = set()\n",
    "    set_dict={}\n",
    "    set_res = {}\n",
    "    print(\"start to sort the results....\")\n",
    "    index = 0\n",
    "    for i, label in enumerate(labels):\n",
    "        if not label in set_dict:\n",
    "#             x, y = coords[i, :]\n",
    "            set_dict[label] = coords[index, :]\n",
    "            index = index+1\n",
    "#             print(label, coords[i, :])\n",
    "        \n",
    "    for k,v in set_dict.items():\n",
    "        min_distance = float('inf')\n",
    "        for k1,v1 in set_dict.items():\n",
    "            dis = distance(v,v1)\n",
    "            if (dis < min_distance) and (not dis == 0):\n",
    "                set_res[k] = k1 \n",
    "                min_distance = dis\n",
    "            else:\n",
    "                continue\n",
    "    print(set_res)\n",
    "    return set_res\n",
    "        \n",
    "#         plt.scatter(x, y)\n",
    "#         plt.annotate(label,\n",
    "#                      xy=(x, y),\n",
    "#                      xytext=(10, 4),\n",
    "#                      textcoords=\"offset points\",\n",
    "#                      ha=\"right\",\n",
    "#                      va=\"bottom\")\n",
    "\n",
    "    #plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-housing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
